{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b2389ef-fce8-4130-841f-100a05d265ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import input_file_name\n",
    "\n",
    "conf = (SparkConf().setMaster(\"k8s://https://192.168.219.100:6443\") # Your master address name\n",
    "        .set(\"spark.kubernetes.container.image\", \"joron1827/pyspark:v2\") # Spark image name\n",
    "        .set(\"spark.driver.port\", \"2222\") # Needs to match svc\n",
    "        .set(\"spark.driver.blockManager.port\", \"7777\")\n",
    "        .set(\"spark.driver.host\", \"driver-service.jupyterhub.svc.cluster.local\") # Needs to match svc\n",
    "        .set(\"spark.driver.bindAddress\", \"0.0.0.0\")\n",
    "        .set(\"spark.kubernetes.namespace\", \"spark\")\n",
    "        .set(\"spark.kubernetes.authenticate.driver.serviceAccountName\", \"spark\")\n",
    "        .set(\"spark.kubernetes.authenticate.serviceAccountName\", \"spark\")\n",
    "        .set(\"spark.executor.instances\", \"4\")\n",
    "        .set(\"spark.kubernetes.container.image.pullPolicy\", \"IfNotPresent\")\n",
    "        .set(\"spark.app.name\", \"joronSpark\")\n",
    "        .set(\"spark.executor.cores\", \"4\")\n",
    "        .set(\"spark.executor.memory\", \"16g\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ddcdb70-c555-4e10-a255-09d00faa208b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/.local/lib/python3.9/site-packages/pyspark/bin/load-spark-env.sh: line 68: ps: command not found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/06/02 04:41:18 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()\n",
    "\n",
    "import time\n",
    "\n",
    "time.sleep(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9bc87f8c-bdd3-47e5-8607-d9bcc0c27b18",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+-------------------------------+----------+-----------------------------+--------------------+--------------------+--------------------+\n",
      "|  code|      date|                          title|New_Column|                       tokens|                  tf|               tfidf|            word2vec|\n",
      "+------+----------+-------------------------------+----------+-----------------------------+--------------------+--------------------+--------------------+\n",
      "|035720|2017-09-26|                               |         1|                           []|      (262144,[],[])|      (262144,[],[])|         (100,[],[])|\n",
      "|003550|2022-03-16|                      주주에...|         0|   [주주, 원한, 잇는건, 가요]|(262144,[51217,78...|(262144,[51217,78...|[0.01652339892461...|\n",
      "|323410|2021-08-09|                           이이|         1|                       [이이]|(262144,[104757],...|(262144,[104757],...|[0.03197551518678...|\n",
      "|007700|2020-06-26|                        와 소름|         0|                       [소름]|(262144,[121178],...|(262144,[121178],...|[-0.0952121093869...|\n",
      "|007460|2020-11-25|                           전환|         1|                       [전환]|(262144,[166149],...|(262144,[166149],...|[-0.1265893280506...|\n",
      "|066570|2018-12-28|             마지막 거래일 매동|         0|           [마지막, 거래, 동]|(262144,[97891,12...|(262144,[97891,12...|[-0.3128598978122...|\n",
      "|006400|2021-01-28|                   삼성전기  등|         1|                 [삼성, 전기]|(262144,[43121,20...|(262144,[43121,20...|[0.09944011270999...|\n",
      "|068270|2020-10-05|               육개장 내오거라 |         0|       [육개장, 내, 오거, 라]|(262144,[60764,74...|(262144,[60764,74...|[0.19440088607370...|\n",
      "|095720|2019-07-10|       입찰 참여 확실시되고 ...|         0| [입찰, 참여, 확, 실시, 되...|(262144,[7578,135...|(262144,[7578,135...|[0.11630692619543...|\n",
      "|005930|2019-12-27|                    너두 기다려|         1|                 [두, 기다려]|(262144,[136803,2...|(262144,[136803,2...|[0.20550757646560...|\n",
      "|047810|2019-10-04|             년말 영업이익 폭증|         0|     [년말, 영업, 이익, 폭증]|(262144,[54114,78...|(262144,[54114,78...|[0.41924566030502...|\n",
      "|010580|2018-03-22|           놀자 오빠가 무리할게|         1|       [놀자, 오빠, 무리할게]|(262144,[26556,13...|(262144,[26556,13...|[0.00531056399146...|\n",
      "|002150|2019-01-22|    단독보도 스웨덴 에서회의...|         1|[단독, 보도, 스웨덴, 회의,...|(262144,[36552,40...|(262144,[36552,40...|[0.06814633003986...|\n",
      "|005930|2023-01-04|                 단타반복질고려|         0|       [단, 타, 반복, 질고려]|(262144,[91473,11...|(262144,[91473,11...|[0.09954914823174...|\n",
      "|011170|2019-05-03|         소리들 그만하고 즐겨라|         0|     [소리, 그만하고, 즐겨라]|(262144,[21648,26...|(262144,[21648,26...|[0.01513559650629...|\n",
      "|008700|2018-02-01|                        시간외 |         1|                         [외]|(262144,[218214],...|(262144,[218214],...|[-0.1454489082098...|\n",
      "|005930|2019-12-27|             쎄꼬랑 찰것 같은데|         1|     [쎄꼬, 랑, 찰것, 같은데]|(262144,[18958,50...|(262144,[18958,50...|[-0.0606460943818...|\n",
      "|010050|2019-11-14|            안씨  형ㅈ들 욕본다|         1|     [안씨, 형, ㅈ, 욕, 본다]|(262144,[22267,55...|(262144,[22267,55...|[0.00390367065556...|\n",
      "|018250|2018-06-18|           애경산업마감시황 속 |         1| [애경, 산업, 마감, 시, 황...|(262144,[26704,65...|(262144,[26704,65...|[-0.0338763503047...|\n",
      "|019170|2020-04-13|                             와|         1|                           []|      (262144,[],[])|      (262144,[],[])|         (100,[],[])|\n",
      "+------+----------+-------------------------------+----------+-----------------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = spark.read.parquet(\"hdfs://192.168.219.121:9000/crawling_stock_text_merge/two_class_text_vectorized.parquet\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3828ae1c-efe8-4f73-8f13-1f6de195d449",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4:==================================================>      (15 + 2) / 17]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/06/02 04:42:09 WARN InstanceBuilder$NativeBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "23/06/02 04:42:09 WARN InstanceBuilder$NativeBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.ForeignLinkerBLAS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/06/02 04:42:09 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS\n",
      "23/06/02 04:42:09 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/06/02 04:42:38 WARN DAGScheduler: Broadcasting large task binary with size 1248.8 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+--------------------+\n",
      "|New_Column|prediction|         probability|\n",
      "+----------+----------+--------------------+\n",
      "|         1|       1.0|[0.13685368423321...|\n",
      "|         1|       1.0|[0.24140023628268...|\n",
      "|         1|       1.0|[0.38090712684037...|\n",
      "|         1|       1.0|[0.16827606561352...|\n",
      "|         1|       1.0|[0.00114636375802...|\n",
      "|         1|       1.0|[0.26074411303739...|\n",
      "|         1|       1.0|[0.38434862811889...|\n",
      "|         0|       1.0|[1.77485957549464...|\n",
      "|         0|       0.0|[0.68700323968091...|\n",
      "|         1|       1.0|[0.16591234548293...|\n",
      "|         1|       0.0|[0.99999995568491...|\n",
      "|         0|       1.0|[0.49689513079298...|\n",
      "|         0|       1.0|[0.18217071543739...|\n",
      "|         0|       1.0|[0.27674792946181...|\n",
      "|         0|       0.0|[0.65777770066362...|\n",
      "|         0|       1.0|[0.30623734010214...|\n",
      "|         0|       0.0|[0.56322801549791...|\n",
      "|         0|       1.0|[0.11961904174933...|\n",
      "|         1|       1.0|[0.19011011870991...|\n",
      "|         0|       0.0|[0.58287078648698...|\n",
      "+----------+----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "23/06/02 04:42:39 WARN DAGScheduler: Broadcasting large task binary with size 1245.7 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/06/02 04:42:43 WARN DAGScheduler: Broadcasting large task binary with size 1264.9 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 182:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+------------------------------------+----------+------------------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|  code|      date|                               title|New_Column|                        tokens|                  tf|               tfidf|            word2vec|       rawPrediction|         probability|prediction|\n",
      "+------+----------+------------------------------------+----------+------------------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|000020|2020-06-12|   요기 고수가 어딧냐 ㄱ 초보들뿐...|         1|  [요기, 고수, 어딧, 냐, ㄱ...|(262144,[43344,77...|(262144,[43344,77...|[-0.0527202452596...|[-1.8416718630815...|[0.13685368423321...|       1.0|\n",
      "|000020|2020-07-29|                   동화약품과 제테마|         1|              [동화약품, 테마]|(262144,[156516,1...|(262144,[156516,1...|[0.15325093083083...|[-1.1450180303182...|[0.24140023628268...|       1.0|\n",
      "|000020|2020-07-29|오늘 공시가 안떠도 우상향하면 좋겠다|         1|  [오늘, 공시, 안, 떠도, 우...|(262144,[1204,104...|(262144,[1204,104...|[0.10966609744355...|[-0.4856997150064...|[0.38090712684037...|       1.0|\n",
      "|000020|2020-07-29|오늘 화이자 제테마 관련인데 이정도면|         1| [오늘, 화이자, 테마, 관련,...|(262144,[43344,64...|(262144,[43344,64...|[0.17477953646864...|[-1.5978946976800...|[0.16827606561352...|       1.0|\n",
      "|000020|2020-08-10|         이번주에     부광  잡는다야|         1|          [주, 부광, 잡는다야]|(262144,[126062,1...|(262144,[126062,1...|[0.04743090107028...|[-6.7700132743379...|[0.00114636375802...|       1.0|\n",
      "|000020|2020-08-10|                    조용히 올라가자 |         1|            [조용히, 올라가자]|(262144,[102391,2...|(262144,[102391,2...|[0.10151316970586...|[-1.0421046055385...|[0.26074411303739...|       1.0|\n",
      "|000020|2020-08-10|         찐찐찐찐 찐이야 완전 찐이야|         1|   [찐, 찐, 찐, 찐, 찐이야,...|(262144,[134677,2...|(262144,[134677,2...|[-0.1195380261966...|[-0.4711308221471...|[0.38434862811889...|       1.0|\n",
      "|000020|2020-10-30|   하한가 하한가 신나는 노랭 나도...|         0| [한가, 한가, 신나는, 노랭,...|(262144,[25535,53...|(262144,[25535,53...|[0.17908089288643...|[-22.452129622362...|[1.77485957549464...|       1.0|\n",
      "|000020|2021-02-04|              공매도 치는 개관들이게|         0|  [공매도, 치는, 개관, 들이게]|(262144,[10185,23...|(262144,[10185,23...|[-0.1664010882377...|[0.78614616783566...|[0.68700323968091...|       0.0|\n",
      "|000020|2021-04-28|  삭제된 게시물의 답글삭제된 게시...|         1|  [삭제, 된, 게시, 물의, 답...|(262144,[73741,12...|(262144,[73741,12...|[0.18557456860850...|[-1.6148788885401...|[0.16591234548293...|       1.0|\n",
      "|000020|2021-04-28|외인 나가려는 순간 주포가 쳐들어온다|         1|[외인, 나가려는, 순간, 주포...|(262144,[5402,966...|(262144,[5402,966...|[-0.0297225018342...|[16.9319405461280...|[0.99999995568491...|       0.0|\n",
      "|000020|2021-11-15|                              먼일이|         0|                        [먼일]|(262144,[37767],[...|(262144,[37767],[...|[0.11389966309070...|[-0.0124196364669...|[0.49689513079298...|       1.0|\n",
      "|000020|2021-11-15| 신규확진 일째 명대 수도권 병상 비상|         0|  [신규, 확진, 째, 명대, 수...|(262144,[57429,63...|(262144,[57429,63...|[0.02537338116339...|[-1.5017093719020...|[0.18217071543739...|       1.0|\n",
      "|000020|2021-11-15| 읽어주세요 강성주주 여러분들께 고함|         0|[읽어주세요, 강성, 주주, 께...|(262144,[21139,62...|(262144,[21139,62...|[-0.2070498961955...|[-0.9606507167358...|[0.27674792946181...|       1.0|\n",
      "|000020|2021-11-25|   이거는 도대체 오르는 날이 없어...|         0|  [거, 는, 도대체, 오르는, ...|(262144,[36552,91...|(262144,[36552,91...|[0.17674064310267...|[0.65340650934252...|[0.65777770066362...|       0.0|\n",
      "|000020|2022-04-13|                         쥐꼬리 망초|         0|                [쥐꼬리, 망초]|(262144,[34348,43...|(262144,[34348,43...|[-0.0032376507297...|[-0.8177694908330...|[0.30623734010214...|       1.0|\n",
      "|000020|2022-05-13|  북한에 감기약 보내야 하는거 아닌가|         0|  [북한, 감기, 약, 보내야, ...|(262144,[52339,99...|(262144,[52339,99...|[0.14478707313537...|[0.25427326270429...|[0.56322801549791...|       0.0|\n",
      "|000040|2019-12-11|    연상 갈 수 있는 분명한 방법이...|         0|  [연상, 갈, 수, 있는, 분명...|(262144,[42636,84...|(262144,[42636,84...|[0.02899918785052...|[-1.9960426801034...|[0.11961904174933...|       1.0|\n",
      "|000040|2020-12-04|   수도권역대급  환자발생  배달오...|         1|  [수도권, 역대, 급, 환자, ...|(262144,[16075,35...|(262144,[16075,35...|[0.10853295607699...|[-1.4492948129691...|[0.19011011870991...|       1.0|\n",
      "|000040|2021-02-05|       공시내용 호재거리 별로 없고마|         0|  [공시, 내용, 호재, 거리, ...|(262144,[1204,947...|(262144,[1204,947...|[0.18924420544256...|[0.33456948791707...|[0.58287078648698...|       0.0|\n",
      "+------+----------+------------------------------------+----------+------------------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Area Under ROC: 0.6048903167931388\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "# 학습 데이터와 테스트 데이터 분할 (70% 학습, 30% 테스트)\n",
    "train_data, test_data = df.randomSplit([0.7, 0.3], seed=42)\n",
    "\n",
    "# 로지스틱 회귀 모델 초기화\n",
    "lr = LogisticRegression(labelCol=\"New_Column\", featuresCol=\"tfidf\")\n",
    "\n",
    "# 모델 학습\n",
    "model = lr.fit(train_data)\n",
    "\n",
    "# 테스트 데이터에 대한 예측\n",
    "predictions = model.transform(test_data)\n",
    "\n",
    "# 예측 결과 확인\n",
    "predictions.select(\"New_Column\", \"prediction\", \"probability\").show()\n",
    "\n",
    "# 이진 분류 평가 지표 계산 (예시로는 Area Under ROC를 사용)\n",
    "evaluator = BinaryClassificationEvaluator(labelCol=\"New_Column\")\n",
    "auc = evaluator.evaluate(predictions)\n",
    "\n",
    "predictions.show()\n",
    "print(\"Area Under ROC:\", auc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "10939afe-b962-41bb-a5c8-a013cf7d169c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/06/02 04:43:38 WARN DAGScheduler: Broadcasting large task binary with size 2.6 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 188:===================================================>   (16 + 1) / 17]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/06/02 04:43:51 WARN DAGScheduler: Broadcasting large task binary with size 1033.7 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/06/02 04:43:51 WARN DAGScheduler: Broadcasting large task binary with size 3.5 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:KeyboardInterrupt while sending command.               (0 + 16) / 17]\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jovyan/.local/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/home/jovyan/.local/lib/python3.9/site-packages/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "  File \"/usr/local/lib/python3.9/socket.py\", line 704, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [5], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m pipeline \u001b[38;5;241m=\u001b[39m Pipeline(stages\u001b[38;5;241m=\u001b[39m[rf])\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# 모델을 학습시킵니다\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# trainData는 학습에 사용할 데이터 프레임입니다\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# 학습된 모델을 사용하여 예측을 수행합니다\u001b[39;00m\n\u001b[1;32m     14\u001b[0m predictions \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mtransform(test_data)  \u001b[38;5;66;03m# testData는 예측에 사용할 데이터 프레임입니다\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/pyspark/ml/base.py:205\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(params)\u001b[38;5;241m.\u001b[39m_fit(dataset)\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 205\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    208\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    209\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(params)\n\u001b[1;32m    210\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/pyspark/ml/pipeline.py:134\u001b[0m, in \u001b[0;36mPipeline._fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    132\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m stage\u001b[38;5;241m.\u001b[39mtransform(dataset)\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# must be an Estimator\u001b[39;00m\n\u001b[0;32m--> 134\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mstage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    135\u001b[0m     transformers\u001b[38;5;241m.\u001b[39mappend(model)\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m<\u001b[39m indexOfLastEstimator:\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/pyspark/ml/base.py:205\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(params)\u001b[38;5;241m.\u001b[39m_fit(dataset)\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 205\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    208\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    209\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(params)\n\u001b[1;32m    210\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/pyspark/ml/wrapper.py:383\u001b[0m, in \u001b[0;36mJavaEstimator._fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    382\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_fit\u001b[39m(\u001b[38;5;28mself\u001b[39m, dataset: DataFrame) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m JM:\n\u001b[0;32m--> 383\u001b[0m     java_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_java\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    384\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_model(java_model)\n\u001b[1;32m    385\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_copyValues(model)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/pyspark/ml/wrapper.py:380\u001b[0m, in \u001b[0;36mJavaEstimator._fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    377\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_java_obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    379\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transfer_params_to_java()\n\u001b[0;32m--> 380\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_java_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/py4j/java_gateway.py:1320\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1313\u001b[0m args_command, temp_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_args(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m-> 1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1322\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/py4j/java_gateway.py:1038\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1036\u001b[0m connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_connection()\n\u001b[1;32m   1037\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1038\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1039\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m binary:\n\u001b[1;32m   1040\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m response, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_connection_guard(connection)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/py4j/clientserver.py:511\u001b[0m, in \u001b[0;36mClientServerConnection.send_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m    509\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    510\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 511\u001b[0m         answer \u001b[38;5;241m=\u001b[39m smart_decode(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m    512\u001b[0m         logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnswer received: \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(answer))\n\u001b[1;32m    513\u001b[0m         \u001b[38;5;66;03m# Happens when a the other end is dead. There might be an empty\u001b[39;00m\n\u001b[1;32m    514\u001b[0m         \u001b[38;5;66;03m# answer before the socket raises an error.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/socket.py:704\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    703\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 704\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    706\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 190:>                                                      (0 + 16) / 17]\r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# RandomForestClassifier 모델을 생성합니다\n",
    "rf = RandomForestClassifier(labelCol=\"New_Column\", featuresCol=\"tfidf\")\n",
    "\n",
    "# 파이프라인을 생성합니다\n",
    "pipeline = Pipeline(stages=[rf])\n",
    "\n",
    "# 모델을 학습시킵니다\n",
    "model = pipeline.fit(train_data)  # trainData는 학습에 사용할 데이터 프레임입니다\n",
    "\n",
    "# 학습된 모델을 사용하여 예측을 수행합니다\n",
    "predictions = model.transform(test_data)  # testData는 예측에 사용할 데이터 프레임입니다\n",
    "\n",
    "# 예측 결과 확인\n",
    "predictions.select(\"New_Column\", \"prediction\", \"probability\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "292f80c2-5e54-4fb9-bbe9-d17a2ec61155",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 199:================================================>      (15 + 2) / 17]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6551456158935797\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# MulticlassClassificationEvaluator를 사용하여 정확도를 계산합니다\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"New_Column\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "09016051-99c8-4ab6-945a-e9aa36e8f06c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/06/02 04:45:41 WARN DAGScheduler: Broadcasting large task binary with size 1257.8 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:KeyboardInterrupt while sending command.>               (0 + 0) / 17]\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jovyan/.local/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/home/jovyan/.local/lib/python3.9/site-packages/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "  File \"/usr/local/lib/python3.9/socket.py\", line 704, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [6], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m evaluator \u001b[38;5;241m=\u001b[39m MulticlassClassificationEvaluator(labelCol\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNew_Column\u001b[39m\u001b[38;5;124m\"\u001b[39m, predictionCol\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprediction\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# 정밀도 계산\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m precision \u001b[38;5;241m=\u001b[39m \u001b[43mevaluator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredictions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43mevaluator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmetricName\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mweightedPrecision\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# 재현율 계산\u001b[39;00m\n\u001b[1;32m     10\u001b[0m recall \u001b[38;5;241m=\u001b[39m evaluator\u001b[38;5;241m.\u001b[39mevaluate(predictions, {evaluator\u001b[38;5;241m.\u001b[39mmetricName: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweightedRecall\u001b[39m\u001b[38;5;124m\"\u001b[39m})\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/pyspark/ml/evaluation.py:109\u001b[0m, in \u001b[0;36mEvaluator.evaluate\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(params, \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m    108\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m params:\n\u001b[0;32m--> 109\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    111\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_evaluate(dataset)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/pyspark/ml/evaluation.py:148\u001b[0m, in \u001b[0;36mJavaEvaluator._evaluate\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transfer_params_to_java()\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_java_obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 148\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_java_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/py4j/java_gateway.py:1320\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1313\u001b[0m args_command, temp_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_args(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m-> 1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1322\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/py4j/java_gateway.py:1038\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1036\u001b[0m connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_connection()\n\u001b[1;32m   1037\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1038\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1039\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m binary:\n\u001b[1;32m   1040\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m response, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_connection_guard(connection)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/py4j/clientserver.py:511\u001b[0m, in \u001b[0;36mClientServerConnection.send_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m    509\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    510\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 511\u001b[0m         answer \u001b[38;5;241m=\u001b[39m smart_decode(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m    512\u001b[0m         logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnswer received: \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(answer))\n\u001b[1;32m    513\u001b[0m         \u001b[38;5;66;03m# Happens when a the other end is dead. There might be an empty\u001b[39;00m\n\u001b[1;32m    514\u001b[0m         \u001b[38;5;66;03m# answer before the socket raises an error.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/socket.py:704\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    703\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 704\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    706\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 190:===>           (4 + 13) / 17][Stage 192:=============> (15 + 2) / 17]\r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# MulticlassClassificationEvaluator를 사용하여 정밀도, 재현율, F1 스코어를 계산합니다\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"New_Column\", predictionCol=\"prediction\")\n",
    "\n",
    "# 정밀도 계산\n",
    "precision = evaluator.evaluate(predictions, {evaluator.metricName: \"weightedPrecision\"})\n",
    "\n",
    "# 재현율 계산\n",
    "recall = evaluator.evaluate(predictions, {evaluator.metricName: \"weightedRecall\"})\n",
    "\n",
    "# F1 스코어 계산\n",
    "f1_score = evaluator.evaluate(predictions, {evaluator.metricName: \"f1\"})\n",
    "\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 Score:\", f1_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d063a10f-83ff-4072-95b4-88f17d2b4667",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/06/02 04:47:15 WARN DAGScheduler: Broadcasting large task binary with size 3.5 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/06/02 04:47:53 WARN DAGScheduler: Broadcasting large task binary with size 2.6 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 220:========>       (9 + 8) / 17][Stage 226:==============>(16 + 1) / 17]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/06/02 04:48:18 WARN DAGScheduler: Broadcasting large task binary with size 1033.7 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/06/02 04:48:18 WARN DAGScheduler: Broadcasting large task binary with size 3.4 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/06/02 04:51:12 WARN DAGScheduler: Broadcasting large task binary with size 3.6 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/06/02 04:52:59 WARN DAGScheduler: Broadcasting large task binary with size 3.4 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/06/02 04:54:14 WARN DAGScheduler: Broadcasting large task binary with size 3.6 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/06/02 04:55:04 WARN DAGScheduler: Broadcasting large task binary with size 3.4 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/06/02 04:56:55 WARN DAGScheduler: Broadcasting large task binary with size 3.6 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 236:==========>    (12 + 5) / 17][Stage 238:=====>         (6 + 11) / 17]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/06/02 04:57:39 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_555_4 !\n",
      "23/06/02 04:57:39 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_429_6 !\n",
      "23/06/02 04:57:39 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_555_6 !\n",
      "23/06/02 04:57:39 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_429_2 !\n",
      "23/06/02 04:57:39 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_429_14 !\n",
      "23/06/02 04:57:39 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_429_10 !\n",
      "23/06/02 04:57:39 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_555_2 !\n",
      "23/06/02 04:57:39 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_555_0 !\n",
      "23/06/02 04:57:39 ERROR TaskSchedulerImpl: Lost executor 3 on 10.244.3.179: \n",
      "The executor with id 3 exited with exit code 137(SIGKILL, possible container OOM).\n",
      "\n",
      "\n",
      "\n",
      "The API gave the following container statuses:\n",
      "\n",
      "\n",
      "\t container name: spark-kubernetes-executor\n",
      "\t container image: docker.io/joron1827/pyspark:v2\n",
      "\t container state: terminated\n",
      "\t container started at: 2023-06-02T04:41:20Z\n",
      "\t container finished at: 2023-06-02T04:57:39Z\n",
      "\t exit code: 137\n",
      "\t termination reason: OOMKilled\n",
      "      \n",
      "23/06/02 04:57:39 WARN TaskSetManager: Lost task 11.0 in stage 238.0 (TID 2571) (10.244.3.179 executor 3): ExecutorLostFailure (executor 3 exited caused by one of the running tasks) Reason: \n",
      "The executor with id 3 exited with exit code 137(SIGKILL, possible container OOM).\n",
      "\n",
      "\n",
      "\n",
      "The API gave the following container statuses:\n",
      "\n",
      "\n",
      "\t container name: spark-kubernetes-executor\n",
      "\t container image: docker.io/joron1827/pyspark:v2\n",
      "\t container state: terminated\n",
      "\t container started at: 2023-06-02T04:41:20Z\n",
      "\t container finished at: 2023-06-02T04:57:39Z\n",
      "\t exit code: 137\n",
      "\t termination reason: OOMKilled\n",
      "      \n",
      "23/06/02 04:57:39 WARN TaskSetManager: Lost task 9.0 in stage 238.0 (TID 2570) (10.244.3.179 executor 3): ExecutorLostFailure (executor 3 exited caused by one of the running tasks) Reason: \n",
      "The executor with id 3 exited with exit code 137(SIGKILL, possible container OOM).\n",
      "\n",
      "\n",
      "\n",
      "The API gave the following container statuses:\n",
      "\n",
      "\n",
      "\t container name: spark-kubernetes-executor\n",
      "\t container image: docker.io/joron1827/pyspark:v2\n",
      "\t container state: terminated\n",
      "\t container started at: 2023-06-02T04:41:20Z\n",
      "\t container finished at: 2023-06-02T04:57:39Z\n",
      "\t exit code: 137\n",
      "\t termination reason: OOMKilled\n",
      "      \n",
      "23/06/02 04:57:39 WARN TaskSetManager: Lost task 15.0 in stage 238.0 (TID 2573) (10.244.3.179 executor 3): ExecutorLostFailure (executor 3 exited caused by one of the running tasks) Reason: \n",
      "The executor with id 3 exited with exit code 137(SIGKILL, possible container OOM).\n",
      "\n",
      "\n",
      "\n",
      "The API gave the following container statuses:\n",
      "\n",
      "\n",
      "\t container name: spark-kubernetes-executor\n",
      "\t container image: docker.io/joron1827/pyspark:v2\n",
      "\t container state: terminated\n",
      "\t container started at: 2023-06-02T04:41:20Z\n",
      "\t container finished at: 2023-06-02T04:57:39Z\n",
      "\t exit code: 137\n",
      "\t termination reason: OOMKilled\n",
      "      \n",
      "23/06/02 04:57:39 WARN TaskSetManager: Lost task 13.0 in stage 238.0 (TID 2572) (10.244.3.179 executor 3): ExecutorLostFailure (executor 3 exited caused by one of the running tasks) Reason: \n",
      "The executor with id 3 exited with exit code 137(SIGKILL, possible container OOM).\n",
      "\n",
      "\n",
      "\n",
      "The API gave the following container statuses:\n",
      "\n",
      "\n",
      "\t container name: spark-kubernetes-executor\n",
      "\t container image: docker.io/joron1827/pyspark:v2\n",
      "\t container state: terminated\n",
      "\t container started at: 2023-06-02T04:41:20Z\n",
      "\t container finished at: 2023-06-02T04:57:39Z\n",
      "\t exit code: 137\n",
      "\t termination reason: OOMKilled\n",
      "      \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/06/02 05:00:32 WARN DAGScheduler: Broadcasting large task binary with size 3.4 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/06/02 05:03:18 WARN DAGScheduler: Broadcasting large task binary with size 3.4 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 236:(21 + -4) / 17][Stage 238:(21 + -4) / 17][Stage 242:>(0 + 16) / 17]7]\r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# 의사결정트리 모델 생성\n",
    "dt = DecisionTreeClassifier(labelCol=\"New_Column\", featuresCol=\"tfidf\")\n",
    "\n",
    "# 모델 훈련\n",
    "dt_model = dt.fit(train_data)\n",
    "\n",
    "# 테스트 데이터에 대한 예측\n",
    "dt_predictions = dt_model.transform(test_data)\n",
    "\n",
    "# 평가 지표 계산을 위한 MulticlassClassificationEvaluator 생성\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"New_Column\", predictionCol=\"prediction\")\n",
    "\n",
    "# 정확도 계산\n",
    "accuracy = evaluator.evaluate(dt_predictions, {evaluator.metricName: \"accuracy\"})\n",
    "\n",
    "# 정밀도 계산\n",
    "precision = evaluator.evaluate(dt_predictions, {evaluator.metricName: \"weightedPrecision\"})\n",
    "\n",
    "# 재현율 계산\n",
    "recall = evaluator.evaluate(dt_predictions, {evaluator.metricName: \"weightedRecall\"})\n",
    "\n",
    "# F1 스코어 계산\n",
    "f1_score = evaluator.evaluate(dt_predictions, {evaluator.metricName: \"f1\"})\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 Score:\", f1_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "812ad806-3ce8-471e-a3fb-ee0fd930fd3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/06/02 03:56:45 ERROR Instrumentation: org.apache.spark.SparkException: Job 146 cancelled because SparkContext was shut down\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$cleanUpAfterSchedulerStop$1(DAGScheduler.scala:1188)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$cleanUpAfterSchedulerStop$1$adapted(DAGScheduler.scala:1186)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.cleanUpAfterSchedulerStop(DAGScheduler.scala:1186)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onStop(DAGScheduler.scala:2887)\n",
      "\tat org.apache.spark.util.EventLoop.stop(EventLoop.scala:84)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.stop(DAGScheduler.scala:2784)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$stop$11(SparkContext.scala:2095)\n",
      "\tat org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1484)\n",
      "\tat org.apache.spark.SparkContext.stop(SparkContext.scala:2095)\n",
      "\tat org.apache.spark.api.java.JavaSparkContext.stop(JavaSparkContext.scala:550)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2249)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2268)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2293)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1021)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\n",
      "\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1020)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$collectAsMap$1(PairRDDFunctions.scala:738)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.collectAsMap(PairRDDFunctions.scala:737)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.findBestSplits(RandomForest.scala:663)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.runBagged(RandomForest.scala:208)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.run(RandomForest.scala:302)\n",
      "\tat org.apache.spark.ml.classification.RandomForestClassifier.$anonfun$train$1(RandomForestClassifier.scala:161)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\n",
      "\tat org.apache.spark.ml.classification.RandomForestClassifier.train(RandomForestClassifier.scala:138)\n",
      "\tat org.apache.spark.ml.classification.RandomForestClassifier.train(RandomForestClassifier.scala:46)\n",
      "\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:151)\n",
      "\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:115)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "23/06/02 03:56:45 WARN ExecutorPodsWatchSnapshotSource: Kubernetes client has been closed.\n"
     ]
    }
   ],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e488f715-5171-45e4-9f7d-946b4de53c74",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
